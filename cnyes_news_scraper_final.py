import requests
import pandas as pd
import datetime as dt
import json
import time

def scrape_cnyes_news(days=10, page_limit=None):
    """
    å°ˆæ¥­çš„é‰…äº¨ç¶²æ–°èçˆ¬èŸ²
    
    Args:
        days: çˆ¬å–éå»å¹¾å¤©çš„æ–°è (é è¨­10å¤©)
        page_limit: é™åˆ¶é æ•¸ (None = çˆ¬å–æ‰€æœ‰é é¢)
    """
    print("ğŸš€ é–‹å§‹çˆ¬å–é‰…äº¨ç¶²æ–°è...")
    
    data = []
    url = "https://api.cnyes.com/media/api/v1/newslist/category/headline"
    
    # è¨­å®šæ™‚é–“ç¯„åœ
    end_time = dt.datetime.today()
    start_time = end_time - dt.timedelta(days=days)
    
    payload = {
        "page": 1,
        "limit": 30,
        "isCategoryHeadline": 1,
        "startAt": int(start_time.timestamp()),
        "endAt": int(end_time.timestamp())
    }
    
    try:
        # ç¬¬ä¸€æ¬¡è«‹æ±‚ä»¥ç²å–ç¸½é æ•¸
        print(f"ğŸ“¡ ç™¼é€è«‹æ±‚åˆ° API...")
        response = requests.get(url, params=payload, timeout=10)
        response.raise_for_status()
        
        jd = json.loads(response.text)
        total_pages = jd['items']['last_page']
        
        print(f"ğŸ“„ ç¸½å…± {total_pages} é è³‡æ–™")
        
        # è™•ç†ç¬¬ä¸€é 
        data.append(pd.DataFrame(jd['items']['data']))
        print(f"âœ… ç¬¬ 1 é å®Œæˆ ({len(jd['items']['data'])} æ¢æ–°è)")
        
        # è™•ç†å¾ŒçºŒé é¢
        max_pages = min(total_pages, page_limit) if page_limit else total_pages
        
        for i in range(2, max_pages + 1):
            print(f"ğŸ“¡ è™•ç†ç¬¬ {i}/{max_pages} é ...")
            
            payload["page"] = i
            response = requests.get(url, params=payload, timeout=10)
            response.raise_for_status()
            
            jd = json.loads(response.text)
            data.append(pd.DataFrame(jd['items']['data']))
            
            print(f"âœ… ç¬¬ {i} é å®Œæˆ ({len(jd['items']['data'])} æ¢æ–°è)")
            
            # æ·»åŠ å»¶é²é¿å…éæ–¼é »ç¹çš„è«‹æ±‚
            time.sleep(0.5)
        
        # åˆä½µè³‡æ–™
        df = pd.concat(data, ignore_index=True)
        
        # è™•ç†è³‡æ–™
        df = df[['newsId', 'title', 'summary']]
        df['link'] = df['newsId'].apply(lambda x: f'https://m.cnyes.com/news/id/{x}')
        
        # å„²å­˜åˆ° CSV
        output_file = 'news.csv'
        df.to_csv(output_file, encoding='utf-8-sig', index=False)
        
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼")
        print(f"ğŸ“Š ç¸½å…±çˆ¬å– {len(df)} æ¢æ–°è")
        print(f"ğŸ’¾ å·²å„²å­˜åˆ° {output_file}")
        
        # é¡¯ç¤ºç¯„ä¾‹
        print(f"\nğŸ“° æ–°èç¯„ä¾‹ (å‰3æ¢):")
        for i, row in df.head(3).iterrows():
            print(f"  {i+1}. {row['title'][:60]}...")
        
        return df
        
    except requests.RequestException as e:
        print(f"âŒ HTTP è«‹æ±‚éŒ¯èª¤: {e}")
        return None
    except json.JSONDecodeError as e:
        print(f"âŒ JSON è§£æéŒ¯èª¤: {e}")
        return None
    except Exception as e:
        print(f"âŒ å…¶ä»–éŒ¯èª¤: {e}")
        return None

if __name__ == "__main__":
    # åŸ·è¡Œçˆ¬èŸ²
    df = scrape_cnyes_news(days=10)
    
    if df is not None:
        print(f"\nâœ… æˆåŠŸå®Œæˆæ–°èçˆ¬å–ä»»å‹™ï¼")
    else:
        print(f"\nâŒ çˆ¬å–å¤±æ•—ï¼Œè«‹æª¢æŸ¥ç¶²è·¯é€£æ¥å’Œ API ç‹€æ…‹")